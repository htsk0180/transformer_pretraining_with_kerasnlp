{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7GYPXgl1w1h"
      },
      "source": [
        "# KerasNLP ile Transformer'ı sıfırdan eğitmek."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o90ybxYV1w1k"
      },
      "source": [
        "KerasNLP, son teknoloji metin işleme modelleri oluşturmayı kolaylaştırmayı amaçlamaktadır. Bu notebook ile transformer'ları sıfırdan nasıl basit bir şekilde eğiteceğimizi göstereceğiz.\n",
        "\n",
        "Bu kılavuz üç bölüme ayrılmıştır:\n",
        "\n",
        "1. Kurulum\n",
        "2. Ön eğitimli bir Transformer Modeli\n",
        "3. Sınıflandırma görevimizde Transformer modelinde fine-tuning işlemi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joqSR8j-1w1k"
      },
      "source": [
        "## Kurulum\n",
        "\n",
        "Paketlerimizi içeri aktaralım: `keras_nlp`, `keras` ve `tensorflow`.\n",
        "\n",
        "[buraya bir göz atmalısın](https://keras.io/api/mixed_precision/), kısaca eğitimi hızlandırmak için hesaplamalarımızın çoğunu 16 bit (32 bit yerine) float sayılarla çalıştırıyoruz.\n",
        "Bir Transformer'ı eğitmek biraz zaman alabilir, bu yüzden daha hızlı eğitim için tüm engelleri kaldırmak önemlidir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CB1F4iIA1w1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f85ac23-190f-412b-e6a6-37290762936d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.8/466.8 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade keras-nlp tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "33AhM8UD1w1m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
        "keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhi-xmZf1w1m"
      },
      "source": [
        "Sonra dataset'leri indirebiliriz.\n",
        "\n",
        "- [SST-2](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary)   Bir metin\n",
        "sınıflandırma veri seti ve \"nihai hedefimiz\" var. Bu veri kümesi genellikle dil modellerinde kıyaslama yapmak için kullanılır.\n",
        "\n",
        "- [WikiText-103](https://paperswithcode.com/dataset/wikitext-103): Ön eğitim için kullanılan İngilizce wikipedia'dan öne çıkan makaleler koleksiyonu veri seti.\n",
        "\n",
        "Son olarak, daha sonra alt kelime belirteci yapmak için bir WordPiece sözlüğü indireceğiz.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fj2lJHje1w1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421b1ed8-6b44-4fb1-a458-30108313f703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "191984949/191984949 [==============================] - 8s 0us/step\n",
            "Downloading data from https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
            "7439277/7439277 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\n",
            "231508/231508 [==============================] - 0s 1us/step\n"
          ]
        }
      ],
      "source": [
        "# Ön eğitim verilerini indirin.\n",
        "keras.utils.get_file(\n",
        "    origin=\"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "wiki_dir = os.path.expanduser(\"~/.keras/datasets/wikitext-103-raw/\")\n",
        "\n",
        "# İnce ayar verilerini indirin.\n",
        "keras.utils.get_file(\n",
        "    origin=\"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "sst_dir = os.path.expanduser(\"~/.keras/datasets/SST-2/\")\n",
        "\n",
        "# Sözlük verilerini indirin.\n",
        "vocab_file = keras.utils.get_file(\n",
        "    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5LSlG7e1w1n"
      },
      "source": [
        "Ardından, eğitim sırasında kullanacağımız bazı hiperparametreleri tanımlıyoruz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tgmdAiSL1w1n"
      },
      "outputs": [],
      "source": [
        "# Ön işleme parametreleri.\n",
        "PRETRAINING_BATCH_SIZE = 128\n",
        "FINETUNING_BATCH_SIZE = 32\n",
        "SEQ_LENGTH = 128\n",
        "MASK_RATE = 0.25\n",
        "PREDICTIONS_PER_SEQ = 32\n",
        "\n",
        "# Model parametreleri.\n",
        "NUM_LAYERS = 3\n",
        "MODEL_DIM = 256\n",
        "INTERMEDIATE_DIM = 512\n",
        "NUM_HEADS = 4\n",
        "DROPOUT = 0.1\n",
        "NORM_EPSILON = 1e-5\n",
        "\n",
        "# Eğitim parametreleri.\n",
        "PRETRAINING_LEARNING_RATE = 5e-4\n",
        "PRETRAINING_EPOCHS = 8\n",
        "FINETUNING_LEARNING_RATE = 5e-5\n",
        "FINETUNING_EPOCHS = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8cVum7n1w1o"
      },
      "source": [
        "### Veri yükle\n",
        "Verilerimizi [tf.data](https://www.tensorflow.org/guide/data) ile yüklüyoruz, tf.data bize metni simgeleştirme ve ön işleme için giriş işlem hatlarını tanımlamamızı sağlar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX7j80SL1w1p"
      },
      "outputs": [],
      "source": [
        "# SST-2 yüklenmesi.\n",
        "sst_train_ds = tf.data.experimental.CsvDataset(\n",
        "    sst_dir + \"train.tsv\", [tf.string, tf.int32], header=True, field_delim=\"\\t\"\n",
        ").batch(FINETUNING_BATCH_SIZE)\n",
        "sst_val_ds = tf.data.experimental.CsvDataset(\n",
        "    sst_dir + \"dev.tsv\", [tf.string, tf.int32], header=True, field_delim=\"\\t\"\n",
        ").batch(FINETUNING_BATCH_SIZE)\n",
        "\n",
        "# wikitext-103'ü yükleyin ve kısa satırları filtreleyin.\n",
        "wiki_train_ds = (\n",
        "    tf.data.TextLineDataset(wiki_dir + \"wiki.train.raw\")\n",
        "    .filter(lambda x: tf.strings.length(x) > 100)\n",
        "    .batch(PRETRAINING_BATCH_SIZE)\n",
        ")\n",
        "wiki_val_ds = (\n",
        "    tf.data.TextLineDataset(wiki_dir + \"wiki.valid.raw\")\n",
        "    .filter(lambda x: tf.strings.length(x) > 100)\n",
        "    .batch(PRETRAINING_BATCH_SIZE)\n",
        ")\n",
        "\n",
        "\n",
        "print(sst_train_ds.unbatch().batch(4).take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOw4O8HN1w1p"
      },
      "source": [
        "`SST-2` veri setimizin nispeten kısa film incelemesi parçacıkları içerdiğini görebilirsiniz.\n",
        "Amacımız, snippet'in duyarlılığını tahmin etmektir. 1 etiketi pozitif duygu ve 0 negatif duygu etiketini temsil eder. Bu dataset için görevimiz bu etiketleri tahmin etmektir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BAX-mFZ1w1p"
      },
      "source": [
        "### Bir temel oluşturmak\n",
        "\n",
        "Pozitif veya negatif bir ağırlık öğrendiğimiz basit bir kelime çantası modeli eğiteceğiz.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-xYkegui1w1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dbed68-f176-4968-e854-a03d0c6a5996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2105/2105 [==============================] - 16s 5ms/step - loss: 0.6131 - accuracy: 0.6873 - val_loss: 0.5367 - val_accuracy: 0.7569\n",
            "Epoch 2/5\n",
            "2105/2105 [==============================] - 11s 5ms/step - loss: 0.5250 - accuracy: 0.7603 - val_loss: 0.4888 - val_accuracy: 0.7821\n",
            "Epoch 3/5\n",
            "2105/2105 [==============================] - 12s 6ms/step - loss: 0.4787 - accuracy: 0.7869 - val_loss: 0.4682 - val_accuracy: 0.7913\n",
            "Epoch 4/5\n",
            "2105/2105 [==============================] - 12s 6ms/step - loss: 0.4486 - accuracy: 0.8021 - val_loss: 0.4591 - val_accuracy: 0.7982\n",
            "Epoch 5/5\n",
            "2105/2105 [==============================] - 12s 6ms/step - loss: 0.4273 - accuracy: 0.8117 - val_loss: 0.4561 - val_accuracy: 0.7982\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f80eb492430>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Bu katman, giriş cümlemizi aynı boyutta 1'ler ve 0'lardan oluşan bir listeye dönüştürecek.\n",
        "\n",
        "multi_hot_layer = keras.layers.TextVectorization(\n",
        "    max_tokens=4000, output_mode=\"multi_hot\"\n",
        ")\n",
        "multi_hot_layer.adapt(sst_train_ds.map(lambda x, y: x))\n",
        "regression_layer = keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "inputs = keras.Input(shape=(), dtype=\"string\")\n",
        "outputs = regression_layer(multi_hot_layer(inputs))\n",
        "baseline_model = keras.Model(inputs, outputs)\n",
        "baseline_model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "baseline_model.fit(sst_train_ds, validation_data=sst_val_ds, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nZ-9IwM1w1q"
      },
      "source": [
        "## Ön Eğitim\n",
        "\n",
        "Bir `transformer` eğiteceğiz, \n",
        "girdimizdeki her kelimeyi düşük boyutlu bir vektör olarak gömeceğiz.Maskeli Dil adlı denetimsiz bir eğitim hedefi kullanacağız.(MaskedLM).\n",
        "\n",
        "Esasen, büyük bir \"eksik kelimeyi tahmin et\" oyunu oynayacağız. Her giriş için\n",
        "örnekte, girdi verilerimizin %25'ini gizleyeceğiz ve modelimizi, istediğimiz parçaları tahmin edecek şekilde eğiteceğiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2_nzftf1w1r"
      },
      "source": [
        "### MaskedLM görevi için verileri ön işleme\n",
        "\n",
        "MaskedLM görevi için metin ön işlememiz iki aşamada gerçekleşecektir.\n",
        "\n",
        "1. Girdi metnini belirteç kimliklerinin tamsayı dizilerine dönüştürün.\n",
        "2. Tahmin etmek için girdimizdeki belirli konumları maskeleyin.\n",
        "\n",
        "`keras_nlp.tokenizers.Tokenizer` kullanarak\n",
        "metni tamsayı belirteç kimlikleri dizilerine dönüştürebiriz.\n",
        "\n",
        "Özellikle, `keras_nlp.tokenizers.WordPieceTokenizer`'ı kullanacağız.\n",
        "*sub-word*  tokenizasyonu, modelleri büyük ölçekte eğitirken popülerdir. Temel olarak, modelimizin yaygın olmayan sözcüklerden öğrenmesine izin verirken,\n",
        "eğitim setimizdeki her kelime için devasa bir kelime dağarcığı gerektiriyor.\n",
        "\n",
        "Yapmamız gereken ikinci şey, MaskedLM görevi için girdimizi maskelemek. Bunu yapmak için kullanabiliriz\n",
        "`keras_nlp.layers.MaskedLMMaskGenerator`, her birinde rastgele bir jeton seti seçecektir.\n",
        "\n",
        "\n",
        "Tokenizer ve maskeleme katmanının her ikisi de bir çağrı içinde kullanılabilir.\n",
        "[tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map).\n",
        "GPU'muz veya TPU'muz çalışırken CPU'daki her partiyi verimli bir şekilde önceden hesaplamak için `tf.data`'yı kullanabiliriz.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsYc8UCL1w1r"
      },
      "outputs": [],
      "source": [
        "# sequence_length ayarı, token çıktılarını şekillendirecek şekilde kırpacak veya dolduracaktır. pad sequence vs. vs.\n",
        "# (batch_size, SEQ_LENGTH).\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab_file,\n",
        "    sequence_length=SEQ_LENGTH,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        ")\n",
        "# mask_selection_length ayarı, maske çıktılarını şekle göre kırpar veya doldurur\n",
        "# (batch_size, PREDICTIONS_PER_SEQ).\n",
        "masker = keras_nlp.layers.MaskedLMMaskGenerator(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    mask_selection_rate=MASK_RATE,\n",
        "    mask_selection_length=PREDICTIONS_PER_SEQ,\n",
        "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(inputs):\n",
        "    inputs = tokenizer(inputs)\n",
        "    outputs = masker(inputs)\n",
        "    # Maskeleme katmanı çıktılarını bir (features, labels, ve weights) olarak ayırın\n",
        "    # tuple that we can use with keras.Model.fit().\n",
        "    features = {\n",
        "        \"token_ids\": outputs[\"token_ids\"],\n",
        "        \"mask_positions\": outputs[\"mask_positions\"],\n",
        "    }\n",
        "    labels = outputs[\"mask_ids\"]\n",
        "    weights = outputs[\"mask_weights\"]\n",
        "    return features, labels, weights\n",
        "\n",
        "\n",
        "# Önceden işlenmiş partileri CPU üzerinde hareket halindeyken önceden hesaplamak için prefetch() kullanıyoruz.\n",
        "pretrain_ds = wiki_train_ds.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "pretrain_val_ds = wiki_val_ds.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Hücreyi her çalıştırdığınızda maskeler değişecektir.\n",
        "print(pretrain_val_ds.take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1HytI9k1w1s"
      },
      "source": [
        "Yukarıdaki blok, veri kümemizi bir `(features, labels, weights)` demetine ayırır ve bunu\n",
        "doğrudan `keras.Model.fit()` öğesine iletir.\n",
        "\n",
        "İki özelliğimiz var:\n",
        "\n",
        "1. `\"token_ids\"`, bazı tokenlerin maske token kimliğimizle değiştirildiği yer.\n",
        "2. `\"mask_positions\"`,hangi tokenleri maskelediğimizi takip eder.\n",
        "\n",
        "Etiketlerimiz, basitçe maskelediğimiz kimliklerdir.\n",
        "\n",
        "Tüm diziler aynı sayıda maskeye sahip olmayacağından, aynı zamanda\n",
        "\"sample_weight\" tensörü, dolgulu etiketleri kayıp fonksiyonumuzdan kaldırır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJde4OAN1w1s"
      },
      "source": [
        "### Transformer encoder oluşturalım.\n",
        "\n",
        "KerasNLP, hızlı bir şekilde bir Transformer encoder oluşturmak için tüm yapı taşlarını sağlar.\n",
        "\n",
        "Giriş token ids'leri ilk önce gömmek için `keras_nlp.layers.TokenAndPositionEmbedding` kullanıyoruz.\n",
        "Bu katman aynı anda iki embeddingi öğrenir -- biri cümledeki kelimeler için, diğeri bir cümledeki tamsayı konumları için. Çıktı gömme basitçe ikisinin toplamıdır.\n",
        "\n",
        "Sonra bir dizi `keras_nlp.layers.TransformerEncoder` katmanı ekleyebiliriz. Bunlar bir dikkat mekanizması (ardından çok katmanlı bir algılayıcı bloğu) kullanarak Transformer modelinin başarımını artırmak içindir.\n",
        "\n",
        "Bu modelin çıktısı, giriş token id'si başına kodlanmış bir vektör olacaktır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iZZz8jz71w1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff11c70-91f4-41be-8382-1dadf44f7c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 128, 256)         7846400   \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " layer_normalization (LayerN  (None, 128, 256)         512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128, 256)          0         \n",
            "                                                                 \n",
            " transformer_encoder (Transf  (None, 128, 256)         527104    \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tran  (None, 128, 256)         527104    \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            " transformer_encoder_2 (Tran  (None, 128, 256)         527104    \n",
            " sformerEncoder)                                                 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,428,224\n",
            "Trainable params: 9,428,224\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n",
        "\n",
        "# Tokenleri konumsal bir embedding ile gömün.\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    sequence_length=SEQ_LENGTH,\n",
        "    embedding_dim=MODEL_DIM,\n",
        ")\n",
        "outputs = embedding_layer(inputs)\n",
        "\n",
        "# Embedding'imize dropout ve normalization uygulayalım\n",
        "outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
        "outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n",
        "\n",
        "# Bir dizi encoder bloğu ekleyin\n",
        "for i in range(NUM_LAYERS):\n",
        "    outputs = keras_nlp.layers.TransformerEncoder(\n",
        "        intermediate_dim=INTERMEDIATE_DIM,\n",
        "        num_heads=NUM_HEADS,\n",
        "        dropout=DROPOUT,\n",
        "        layer_norm_epsilon=NORM_EPSILON,\n",
        "    )(outputs)\n",
        "\n",
        "encoder_model = keras.Model(inputs, outputs)\n",
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IrA0fLs1w1t"
      },
      "source": [
        "### Transformer'ı önceden eğitin\n",
        "\n",
        "`keras_nlp.layers.MaskedLMHead` kullanarak encoder bloğumuzu MaskedLM görevini yapacak şekilde eğitebiliriz.\n",
        "\n",
        "Bu katman, bir girdi olarak token kodlamalarını ve bir başkası olarak da bizim belirlediğimiz konumları alacaktır.\n",
        "orijinal girişte maskelenmiş. Maskelediğimiz token kodlamalarını toplayacak ve\n",
        "onları tüm kelime dağarcığımız üzerindeki tahminlere geri dönüştürün.\n",
        "\n",
        "Bununla, ön eğitimi derlemeye ve çalıştırmaya hazırız. Bunu bir\n",
        "colab kullanarak, yaklaşık bir saat süreceğini unutmayın. Training Transformer, yoğun hesaplamasıyla ünlüdür. Dolayısıyla bu nispeten küçük Transformatör bile biraz zaman alacaktır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFVjvMtA1w1t"
      },
      "outputs": [],
      "source": [
        "# Maskelenmiş bir dil modeli başlığı ekleyerek eğitim öncesi modeli oluşturun.\n",
        "inputs = {\n",
        "    \"token_ids\": keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32),\n",
        "    \"mask_positions\": keras.Input(shape=(PREDICTIONS_PER_SEQ,), dtype=tf.int32),\n",
        "}\n",
        "\n",
        "# Encode tokens.\n",
        "encoded_tokens = encoder_model(inputs[\"token_ids\"])\n",
        "\n",
        "# Her maskelenmiş giriş tokeni için bir çıkış sözcüğü tahmin edin.\n",
        "# Encoding vektörlerimizden kelime logitlerine yansıtmak için giriş tokenlerini yerleştirmeyi kullanıyoruz, bunun eğitim verimliliğini artırdığı gösterilmiştir.\n",
        "outputs = keras_nlp.layers.MaskedLMHead(\n",
        "    embedding_weights=embedding_layer.token_embedding.embeddings,\n",
        "    activation=\"softmax\",\n",
        ")(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n",
        "\n",
        "# Ön eğitim modelimizi tanımlayın ve derleyin.\n",
        "pretraining_model = keras.Model(inputs, outputs)\n",
        "pretraining_model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.experimental.AdamW(PRETRAINING_LEARNING_RATE),\n",
        "    weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
        "    jit_compile=True,\n",
        ")\n",
        "\n",
        "# Modeli wiki metin veri kümemizde önceden eğitin.\n",
        "pretraining_model.fit(\n",
        "    pretrain_ds,\n",
        "    validation_data=pretrain_val_ds,\n",
        "    epochs=PRETRAINING_EPOCHS,\n",
        ")\n",
        "\n",
        "# SDaha fazla ince ayar için bu temel modeli kaydedin.\n",
        "encoder_model.save(\"encoder_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLB9IJxu1w1t"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Ön eğitimden sonra artık modelimizin ince ayarını \"SST-2\" veri kümesinde yapabiliriz. Bağlam içindeki kelimeleri tahmin etmek için oluşturduğumuz kodlayıcının yeteneğinden yararlanın.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDiKkVdq1w1u"
      },
      "source": [
        "### Verileri sınıflandırma için ön işleme\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St_c42gv1w1u"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess(sentences, labels):\n",
        "    return tokenizer(sentences), labels\n",
        "\n",
        "\n",
        "# Önceden işlenmiş partileri CPU'muzda hareket halindeyken önceden hesaplamak için prefetch() kullanıyoruz.\n",
        "finetune_ds = sst_train_ds.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "finetune_val_ds = sst_val_ds.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "print(finetune_val_ds.take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d3rr0cV1w1u"
      },
      "source": [
        "### Fine-tune Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neKFxZly1w1u"
      },
      "outputs": [],
      "source": [
        "# İnce ayarı sıfırdan yeniden başlatabilmemiz için kodlayıcı modelini diskten yeniden yükleyin.\n",
        "encoder_model = keras.models.load_model(\"encoder_model\", compile=False)\n",
        "\n",
        "# Girdi olarak tokenleştirilmiş girişi alın.\n",
        "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n",
        "\n",
        "# Tokenleri encoder edin ve pooling'leyin.\n",
        "encoded_tokens = encoder_model(inputs)\n",
        "pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens)\n",
        "\n",
        "# Bir çıktı etiketi tahmin edin.\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(pooled_tokens)\n",
        "\n",
        "# Hassas ayar modelimizi tanımlayın ve derleyin.\n",
        "finetuning_model = keras.Model(inputs, outputs)\n",
        "finetuning_model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.experimental.AdamW(FINETUNING_LEARNING_RATE),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Modelde SST-2 görevi için ince ayar yapın.\n",
        "finetuning_model.fit(\n",
        "    finetune_ds,\n",
        "    validation_data=finetune_val_ds,\n",
        "    epochs=FINETUNING_EPOCHS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuFiY3ye1w1u"
      },
      "source": [
        "### Test için bir model kaydedelim.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJRstV9Y1w1u"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(), dtype=tf.string)\n",
        "tokens = tokenizer(inputs)\n",
        "outputs = finetuning_model(tokens)\n",
        "final_model = keras.Model(inputs, outputs)\n",
        "final_model.save(\"final_model\")\n",
        "\n",
        "restored_model = keras.models.load_model(\"final_model\", compile=False)\n",
        "inference_data = tf.constant([\"Terrible, no good, trash.\", \"So great; I loved it!\"])\n",
        "print(restored_model(inference_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ecUGPI1w1u"
      },
      "source": [
        "KerasNLP'nin temel hedeflerinden biri, NLP model oluşturmaya modüler bir yaklaşım sağlamaktır. Burada bir Transformer oluşturmaya yönelik bir yaklaşım gösterdik, ancak KerasNLP, metin ön işleme ve model oluşturma için sürekli büyüyen bir bileşen dizisini destekler. Doğal dil problemlerinize çözümler üzerinde deneyler yapmayı kolaylaştıracağını umuyoruz."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}